{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fraudify_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **'Fraudify.py' - A Convolutional Neural Network based signature verification program**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## *With references to the work of multiple authors of which helped in implementation and/or understanding;*\n",
        "\n",
        "\n",
        "*  Real-Forge-Signature-Detection - GitHub (akhiilkasare, 2020)\n",
        "\n",
        "\n",
        "*   Image Classification Using CNN -Understanding Computer Vision - (Analytics Vidhya 2021)\n",
        "\n",
        "\n",
        "\n",
        "*   CEDAR Dataset \n",
        "*   CNN for Human Activity Recognition - GitHub (aqibsaeed, 2019)\n",
        "\n",
        "\n",
        "*     IUST Pattern Recognition Projects - GitHub (Parsa Abbasi 2021)\n",
        "\n",
        "*   Handwritten Signature - Classification - GitHub (Yasha, G., 2021)\n",
        "\n",
        "\n",
        "*  S3: pytorch Speaker verification - test model - GitHub (VERMA , G, K., 2021)\n",
        " \n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "## *Phase 1: CNN Fraudify.py application*  \n",
        "\n",
        "Creating a CNN based Static Signature Verification application to comparatively assess and analysis its accuracy and optimisation, im comparison to Phase 2 SCNN-based Fraudify application.\n"
      ],
      "metadata": {
        "id": "6bVOoIiYMJc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wqbYLaNpjfn3"
      },
      "outputs": [],
      "source": [
        "#Importing all required libraries for the System\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import skimage.io as sk\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.io import imread\n",
        "from scipy import spatial\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, Lambda, MaxPooling2D, Conv2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from keras.models import Sequential\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime, os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking if GPU is used**"
      ],
      "metadata": {
        "id": "oPwFkTHhack6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to check if GPU is being used\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq02QTiCjk_P",
        "outputId": "8a1e0444-f3ae-48c2-a3f1-b78f41fc4d48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting google drive**"
      ],
      "metadata": {
        "id": "GgCjS8P1aguR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting my drive to google colab for access within the drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "Lp0RK2fGjmw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a093b5-d992-4c21-9ba6-89e49fcdcfcf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the paths for the training and testing data with loaded directories. \n",
        "\n",
        "train_path = '/content/gdrive/MyDrive/FYP/Train/CEDAR_1'\n",
        "test_path = '/content/gdrive/MyDrive/FYP/Test/CEDAR_1'"
      ],
      "metadata": {
        "id": "UuNs7McljoL3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As part of the pre-processing of the data, is creating variables for the width and height dimensions\n",
        "# to make all the data instances of the same. \n",
        "# Creating an image_channel variable with the number 3 as the data instances are RGB.\n",
        "\n",
        "Image_Width = 512\n",
        "Image_Height = 512\n",
        "Image_Size = (Image_Width, Image_Height)\n",
        "Image_Channel = 3\n",
        "batch_size=64"
      ],
      "metadata": {
        "id": "DcJ6sN7Fjpxm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the CNN Model**"
      ],
      "metadata": {
        "id": "GBBHaOd2aqbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN model including multiple convolutional layers, pooling layers and fully connected layer.\n",
        "#2 Dense layers based of Yoshia Bengio recommendation. \n",
        "#There is dropout to optimise and reduce chances of overfitting.\n",
        "#Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase.\n",
        "\n",
        "\n",
        "#def createCNN(input_shape):\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "##input layer\n",
        "## Conv layer 1\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(Image_Width,Image_Height, Image_Channel)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 2\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 3\n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 4\n",
        "model.add(Conv2D(256, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 5\n",
        "#model.add(Conv2D(256, (3,3), activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "## Conv layer 6\n",
        "model.add(Conv2D(512, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "#Fully connected layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RrQr5hJwjrJu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "ZWXoWgMwju2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Establishing Hyper-parameters**"
      ],
      "metadata": {
        "id": "TyBl4M8zcGgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Early stopping is a callback hyperparameter that will stop the training when a the validation loss has stopped improving.\n",
        "#Reduce LR is another keras callback, which allows for the reducing of the Learning Rate where Validation Accracy has no imporvements after 5 epochs. \n",
        "#TensorBoard is a callback that will allow for further diagnostic charts and graphs to be displayed for analysis.\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "early_stop = EarlyStopping(monitor= 'val_loss', patience=10, verbose=0, mode = 'auto')\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.2, min_lr=0.00025)\n",
        "\n",
        "#logdir = os.path.join(\"logs\" , datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "callbacks = [early_stop, learning_rate_reduction, #tensorboard_callback\n",
        "             ]"
      ],
      "metadata": {
        "id": "yUTmCTfQjygH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training data generating - Preprocessing and Data Augmenting**"
      ],
      "metadata": {
        "id": "V2rDV6RUdQsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling all the images between 0 to 1 and applying Data Augmentation.\n",
        "\n",
        "train_datagen = ImageDataGenerator(rotation_range=15,\n",
        "                                  rescale=1./255,\n",
        "                                  shear_range=0.1,\n",
        "                                  zoom_range=0.2,\n",
        "                                  horizontal_flip=True,\n",
        "                                  width_shift_range=0.1,\n",
        "                                  height_shift_range=0.1,)"
      ],
      "metadata": {
        "id": "oz06bhlikEMu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For generating the training, the directory called is the CEDAR Training which has 1269 images across Genuines and Forgeries. (k/fold ration 8:2)\n",
        "#to optimise the batch size, 64 was found to be the most appropriate, increasing the validation accuracies. \n",
        "#Batch size controls the accuracy of the estimate of the error gradient when training neural networks.\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('/content/gdrive/MyDrive/FYP/Train/CEDAR_1',\n",
        "                                              target_size=Image_Size,\n",
        "                                              batch_size=64,\n",
        "                                              class_mode = 'categorical')"
      ],
      "metadata": {
        "id": "t4NbgnmvkFfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31493db3-1070-4f0e-bb05-003100e80609"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1269 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing data generating - Preprocessing (No Data Augmentation)**"
      ],
      "metadata": {
        "id": "Bsm4eyH5dY6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing only scaling on the test dataset and not Data Augmentation\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "P87UogivkHhe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For generating the testing, the directory called is the CEDAR Testing which has 306 images across Genuines and Forgeries. (k/fold ration 8:2)\n",
        "#to optimise the batch size, 64 was found to be the most appropriate, increasing the validation accuracies. \n",
        "#Batch size controls the accuracy of the estimate of the error gradient when training neural networks\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory('/content/gdrive/MyDrive/FYP/Test/CEDAR_1',\n",
        "                                                  target_size=Image_Size,\n",
        "                                                  batch_size = 64,\n",
        "                                                  class_mode='categorical')"
      ],
      "metadata": {
        "id": "0OHfBjp1kJPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d58a8e0-e057-4524-ee47-7348cb7477d0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 306 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training data against model**"
      ],
      "metadata": {
        "id": "iYykXxUrdhOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#With hyperparameter tuning of the epochs, 10 were found to be an optimised number for achieving a consistently growing validation accuracy.\n",
        "#Ensuring that the callbacks are called in the model.fit as well as a number of steps per epochs.\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                             epochs=epochs,\n",
        "                             validation_data=test_generator,\n",
        "                             validation_steps=len(test_generator),\n",
        "                             steps_per_epoch=len(train_generator),\n",
        "                             callbacks=callbacks)"
      ],
      "metadata": {
        "id": "_Y-OtsymkLC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Tensorboard magic tool, for more graphs and histograms on each section within the training.\n",
        "\n",
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "9Tj3vspCkPW2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the trained model and proceeding to save it as 'TrainedSignature_CNNmodel' in h5 format.\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model.save('TrainedSignature_model.h5')"
      ],
      "metadata": {
        "id": "N8QNMa72upj4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the model \n",
        "\n",
        "model = load_model('TrainedSignature_CNNmodel.h5')"
      ],
      "metadata": {
        "id": "czkZASEwjgXD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model prediction for the test directories. \n",
        "pred = model.predict(test_generator)\n",
        "pred"
      ],
      "metadata": {
        "id": "1r9hDUdXpR-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multi-class classification is set. (selecting the class with the highest probability)\n",
        "#numpy Argmax returns the indices of the full values across the axis of the array (in this instance 'pred')\n",
        "#With the axis being set at 1, numpy looks through all the rows within the prediction variable\n",
        "#indexing of the prediction within the argmax results.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "pred = np.argmax(pred, axis=1)\n",
        "\n",
        "#printing the predictions, we can see that numpy has looked through all rows of the array, with the prediction labels set \n",
        "#for each of the data instances within the \n",
        "\n",
        "pred"
      ],
      "metadata": {
        "id": "6stwfuFkuzVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAKE PREDICTIONS**"
      ],
      "metadata": {
        "id": "MF7H2gC5gQxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing image module from keras to load the data for testing.\n",
        "#Assigning the path file of a signature to test and predict - making sure that the size of the image is targeted at 512x512 which is the same as the training and testing data.\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "img = image.load_img('/content/gdrive/MyDrive/FYP/Test/CEDAR_1/forged/forgeries_10_13.png', target_size=(512,512))\n",
        "\n"
      ],
      "metadata": {
        "id": "xUXYIme-u9Ox"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting the PIL image loaded into a numpy array. \n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x"
      ],
      "metadata": {
        "id": "u32HrVTgu_yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the shape of x, which displays the size as 512x512 and the 3 which is the RGB coloring\n",
        "\n",
        "x.shape"
      ],
      "metadata": {
        "id": "e-gGjQc6vSOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The expansion of dimensions before putting the image data through 'preprocess input'\n",
        "#This is because we need an addition dimension to get a batch of the image.\n",
        "#Printing of the image.data.shape shows the addition dimensions for the samples. \n",
        "\n",
        "x = x/255\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "x=np.expand_dims(x,axis=0)\n",
        "img_data=preprocess_input(x)\n",
        "img_data.shape"
      ],
      "metadata": {
        "id": "uPJm4h10vWb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting the image instance that will be tested against the model\n",
        "\n",
        "model.predict(img_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKZp6QTqvYZh",
        "outputId": "8392346f-b2c5-4453-f0c6-0d41f64de982"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For multi-Class classification (such as using Softmax layers) \n",
        "\n",
        "a=np.argmax(model.predict(img_data), axis=1)"
      ],
      "metadata": {
        "id": "oYI2dmWwva_p"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(a==1):\n",
        "    print(\"The signature is Genuine\")\n",
        "else:\n",
        "    print(\"The signature is Forged\")"
      ],
      "metadata": {
        "id": "uMS5O-jVve9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **METRICS OF EVALUATION**"
      ],
      "metadata": {
        "id": "jkDv_ACbd__J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import seaborn as sn\n",
        "\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools"
      ],
      "metadata": {
        "id": "UWJ0KewWeD35"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "#\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "aPChN14xZGSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the validation accuracy and loss.\n",
        "#There is a small difference between the two.\n",
        "\n",
        "\n",
        "score = model.evaluate(test_generator)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])"
      ],
      "metadata": {
        "id": "_eB4Bt-neE26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_true_labels = test_generator.classes"
      ],
      "metadata": {
        "id": "vfE-pfOyeI3Q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Calculating Accuracy and F1- Scores***"
      ],
      "metadata": {
        "id": "07HfxgI7faq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the accuracy and f1-score\n",
        "acc = accuracy_score( test_true_labels   , pred )\n",
        "f1 = f1_score(test_true_labels, pred , average='weighted')\n",
        "print('Test results:', 'accuracy=', acc, ', Weighted F1-score=', f1)"
      ],
      "metadata": {
        "id": "KlKfNphKeK1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Confusion Matrix with Percentages***"
      ],
      "metadata": {
        "id": "WsyzNc9Lfhuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusing Matrix set of Precision, Recall and F1 scores in percentage.\n",
        "\n",
        "\n",
        "print(\"P,R,F1:\",precision_recall_fscore_support(test_true_labels, pred, average='macro'))\n",
        "df_cm = pd.DataFrame(confusion_matrix(test_true_labels, pred,normalize = 'true'), index = [i for i in \"01\"],\n",
        "                     columns = [i for i in \"01\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "metadata": {
        "id": "pUR3LAeXeNAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Establishing variable 'cm' with confusion matrix\n",
        "\n",
        "cm = confusion_matrix(y_true=test_true_labels, y_pred= pred )"
      ],
      "metadata": {
        "id": "oVXkk41nfwyi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "S3DbC0-Sf54y"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the labels as genuine signatures and forged signatures\n",
        "\n",
        "cm_plot_labels = ['Genuine Signatures','Forged Signatures']"
      ],
      "metadata": {
        "id": "2AfkVcDYf-jh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Phase 1 Confusion Matrix')"
      ],
      "metadata": {
        "id": "JOXbsRHVgEwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a figure which displays the training loss, training accuracy, validation loss and validation accuracy. \n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "#plt.plot(history.history['val_loss'], label='Val loss')\n",
        "#plt.plot(history.history['accuracy'], label='Training acc')\n",
        "#plt.plot(history.history['val_accuracy'], label='val acc')\n",
        "plt.title(\"Training Loss and Accuracy on CEDAR Dataset\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('lossval_loss')"
      ],
      "metadata": {
        "id": "T0zIG7yQgBEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "#\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "sFKQg8KZ6CYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Tensorboard magic tool, for more graphs and histograms on each section within the training.\n",
        "\n",
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "IvAao0GhgNxq"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}